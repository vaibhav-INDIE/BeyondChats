# Reddit Persona Analyzer üõ†Ô∏è

Generate deep psychological and behavioral personas from Reddit user profiles using OpenAI models. This tool scrapes a user's public activity, analyzes the content using AI, and outputs a professional PDF report summarizing their inferred interests, personality traits, communication style, values, and more.

---

## üßê How It Works: The Approach

This project follows a multi-step pipeline to transform raw Reddit activity into a structured psychological profile.

1.  **üìú Data Scraping (`scrapper.py`):** The process begins by fetching the user's public comments and submission history using the **PRAW** (Python Reddit API Wrapper). This raw text data forms the foundation of the analysis.

2.  **üß† Vector Database (`db.py`):** Raw text is difficult for AI models to parse efficiently. To solve this, we process the scraped data and embed it into a vector space using OpenAI's text embedding models. These embeddings are stored in a **FAISS** (Facebook AI Similarity Search) index. This creates a highly optimized, searchable knowledge base of the user's activities, allowing for rapid retrieval of relevant information based on semantic meaning rather than just keywords.

3.  **üïµÔ∏è‚Äç‚ôÇÔ∏è Persona Profiling (`profiler.py`):** This is the core analysis step. The profiler queries the FAISS vector database to find the most relevant user comments and posts related to specific psychological and behavioral categories (e.g., "hobbies," "opinion on technology," "social interaction style"). This contextual data is then fed into an advanced OpenAI model (like GPT-4) with a series of carefully crafted prompts. The model synthesizes this information to infer personality traits (e.g., using the Big Five model), communication style, core values, and potential interests. The output is a structured JSON object.

4.  **üìÑ PDF Report Generation (`report.py`):** The structured JSON data from the profiler is used to generate a clean, professional, and easy-to-read PDF report using the **ReportLab** library. This report presents the key findings in a digestible format, making the insights accessible to anyone.

5.  **‚öôÔ∏è Master Pipeline (`main.py`):** This script orchestrates the entire workflow. It takes a Reddit profile URL as input and sequentially runs the scraper, database builder, profiler, and report generator, ensuring a seamless process from start to finish.

---

## ‚öôÔ∏è Setup Instructions

#### 1Ô∏è‚É£ Clone the Repository
git clone [https://github.com/your-username/reddit-persona-analyzer.git](https://github.com/your-username/reddit-persona-analyzer.git)
cd reddit-persona-analyzer

2Ô∏è‚É£ Create Python Virtual Environment (Optional but Recommended)

Linux / macOS


python3 -m venv venv
source venv/bin/activate
Windows


python -m venv venv
venv\Scripts\activate

3Ô∏è‚É£ Install Dependencies


pip install -r requirements.txt

4Ô∏è‚É£ Set Up Environment Variables
Create a file named .env in the project root directory:

touch .env

Add your API keys to the .env file. This keeps your sensitive credentials secure and out of version control.

# Reddit API credentials (create an app at [https://www.reddit.com/prefs/apps](https://www.reddit.com/prefs/apps))
REDDIT_CLIENT_ID=your_reddit_client_id
REDDIT_CLIENT_SECRET=your_reddit_client_secret
USER_AGENT=YourAppName/1.0

# OpenAI API key (get from [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys))

OPENAI_API_KEY=your_openai_api_key

üöÄ How to Run
Run the complete pipeline with a single command:

python main.py
You will be prompted to enter the Reddit profile URL you wish to analyze.

Example Input:
[https://www.reddit.com/user/spez/](https://www.reddit.com/user/spez/)

‚úÖ Output:
Upon completion, the following files will be generated in your project directory:
spez_data.json: Raw scraped data from the user's profile.
spez_reddit.faiss: The FAISS vector database file.
spez_deep_persona.json: The detailed persona generated by the OpenAI model.
spez_persona_report.pdf: The final, professional PDF report.

üõ† Dependencies
praw: Python Reddit API Wrapper for data scraping.
openai: Official library for accessing OpenAI's language models.
faiss: For efficient similarity search in the vector database.
reportlab: For programmatically creating the PDF report.
python-dotenv: To load environment variables from the .env file.
tiktoken: For token counting to manage API costs.

(All dependencies are automatically installed by running pip install -r requirements.txt)

‚úçÔ∏è Credits
Developed by Vaibhav Jain
Powered by the OpenAI API and the Reddit API.

‚ö†Ô∏è Disclaimer
This project is intended for educational and research purposes only. The generated personas are based on inferences from public data and may not be accurate. Use this tool responsibly and ethically. Do not misuse scraped Reddit data or use this tool for malicious purposes. Respect user privacy.
